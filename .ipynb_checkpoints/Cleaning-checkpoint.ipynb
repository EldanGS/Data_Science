{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Add libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import re\n",
    "import codecs\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import operator\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove all stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeall(data, stopWords):\n",
    "    for x in stopWords:\n",
    "        if x in data:\n",
    "            for item in range(data.count(x)):\n",
    "                data.remove(x)\n",
    "    return data                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create object Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = Mystem() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization with pymystem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatization(data):\n",
    "    lemmas = m.lemmatize(data)\n",
    "    return ''.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open JsonFile in directory\n",
    "with open(\"Sample_Input.json\") as data_file:\n",
    "    data = json.loads(data_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# original stopwords (151)\n",
    "stopWords = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add new 'Vk popular' stop words (195)\n",
    "badwords = [\n",
    "u'я', u'а', u'да', u'но', u'тебе', u'мне', u'ты', u'и', u'у', u'на', u'ща', u'ага',\n",
    "u'так', u'там', u'какие', u'который', u'какая', u'туда', u'давай', u'короче', u'кажется', u'вообще',\n",
    "u'ну', u'не', u'чет', u'неа', u'свои', u'наше', u'хотя', u'такое', u'например', u'кароч', u'как-то',\n",
    "u'нам', u'хм', u'всем', u'нет', u'да', u'оно', u'своем', u'про', u'вы', u'м', u'тд',\n",
    "u'вся', u'кто-то', u'что-то', u'вам', u'это', u'эта', u'эти', u'этот', u'прям', u'либо', u'как', u'мы',\n",
    "u'просто', u'блин', u'очень', u'самые', u'твоем', u'ваша', u'кстати', u'вроде', u'типа', u'пока', u'ок',u'в'\n",
    ",u'б',u'г',u'д',u'е',u'ж',u'з',u'й',u'к',u'л',u'ф',u'н',u'о',u'п',u'р',u'с',u'т',u'ч',u'ц',u'ч',u'ш',u'щ',u'ь'\n",
    ",u'ъ',u'ы',u'э','ю']\n",
    "\n",
    "for words in badwords:\n",
    "    stopWords.add(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral  13\n",
      "positive 6\n",
      "negative 3\n"
     ]
    }
   ],
   "source": [
    "# checking amount of sentiment data\n",
    "neutral = 0\n",
    "positive = 0\n",
    "negative = 0\n",
    "for i in range (0, len(data)):\n",
    "    if data[i][\"manual_sentiment\"] == \"neutral\":\n",
    "        neutral +=1\n",
    "    elif data[i][\"manual_sentiment\"] == \"positive\":\n",
    "        positive +=1\n",
    "    else:\n",
    "        negative +=1       \n",
    "print ('neutral ',neutral)\n",
    "print ('positive',positive)\n",
    "print ('negative',negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cleaning form non-alphabet letters, spliting into words, remove all stopWords\n",
    "#dict_set = set()\n",
    "small_words = list()\n",
    "need_words = (u'мир', u'суд', u'ак')\n",
    "for i in range (0,len(data)):    \n",
    "    data[i][\"text\"] = re.sub(\"[^а-яА-ЯЁё]\",\" \", data[i][\"text\"]) \n",
    "    data[i][\"text\"] = lemmatization(data[i][\"text\"])\n",
    "    data[i][\"text\"] = data[i][\"text\"].lower().split()\n",
    "    data[i][\"text\"] = removeall(data[i][\"text\"], stopWords)\n",
    "    for word in data[i][\"text\"]:\n",
    "        if len(word) < 4:\n",
    "            small_words.append(word)\n",
    "    for x in small_words:\n",
    "        if (x in data[i][\"text\"]) and (x not in need_words):\n",
    "            data[i][\"text\"].remove(x)\n",
    "    small_words.clear()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# deleting duplicates from list O(n^2)\n",
    "new_list = list()\n",
    "cnt = 0\n",
    "for i in data:\n",
    "    if (i not in new_list):\n",
    "        new_list.append(i)\n",
    "    else:\n",
    "        cnt+=1\n",
    "print (cnt)\n",
    "\n",
    "\n",
    "data.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#delete texts if it is similar but sentiment different\n",
    "new_list2 = list()\n",
    "cnt = 0\n",
    "for i in new_list:\n",
    "    check = False\n",
    "    for j in new_list:\n",
    "        if (i[\"text\"] == j[\"text\"]) and ( i[\"manual_sentiment\"] != j[\"manual_sentiment\"] ):\n",
    "            check = True\n",
    "            cnt+=1\n",
    "    if check == False:\n",
    "        new_list2.append(i)\n",
    "        \n",
    "new_list.clear()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "5\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# checking amount of sentiment data after cleaning\n",
    "neutral = 0\n",
    "positive = 0\n",
    "negative = 0\n",
    "for i in range (0, len(new_list2)):\n",
    "    if new_list2[i][\"manual_sentiment\"] == \"neutral\":\n",
    "        neutral +=1\n",
    "    elif new_list2[i][\"manual_sentiment\"] == \"positive\":\n",
    "        positive +=1\n",
    "    else:\n",
    "        negative +=1\n",
    "print (neutral) \n",
    "print (positive)\n",
    "print (negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# writing into json file \n",
    "with open (\"Sample_Output.json\", \"w\", encoding='utf8') as outfile:\n",
    "    outfile.write('[')\n",
    "    for i in range(0,len(new_list2)):\n",
    "        json.dump(new_list2[i], outfile, ensure_ascii=False)\n",
    "        if i+1 < len(new_list2):\n",
    "            outfile.write(',')\n",
    "        outfile.write('\\n')\n",
    "    outfile.write(']')   "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
